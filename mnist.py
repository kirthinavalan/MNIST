# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mgr2q72OTIuOTY8aNQnRUbF3AIYI5Yds
"""



import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

"""0- grey
255-white, 28 x 28 matrix
"""

# loading data set
mnist_data, mnist_info =tfds.load(name="mnist" , with_info=True, as_supervised=True)

mnist_train, mnist_test=mnist_data['train'],mnist_data['test']
num_validation_samples=0.1*mnist_info.splits['train'].num_examples
num_validation_samples=tf.cast(num_validation_samples,tf.int64)
num_test_samples=mnist_info.splits['test'].num_examples
num_test_samples=tf.cast(num_test_samples,tf.int64)

def scale(image,label):
  image=tf.cast(image,tf.float32)
  image/=255.
  return image, label

scaled_train_and_validation_data = mnist_train.map(scale)

test_data = mnist_test.map(scale)

# shuffle the data

BUFFER_SIZE=10000

shuffed_train_and_validation_data=  scaled_train_and_validation_data.shuffle(BUFFER_SIZE)

validation_data= shuffed_train_and_validation_data.take(num_validation_samples)
train_data=shuffed_train_and_validation_data.skip(num_validation_samples)

# batching

BATCH_SIZE=100
train_data =train_data.batch(BATCH_SIZE)
validation_data=validation_data.batch(num_validation_samples)
test_data= test_data.batch(num_test_samples)

validated_inputs , validated_targets = next(iter(validation_data))

mnist_info

# outline the model
input_size=784
output_size=10
hidden_layer_size=200
model=tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28,1)),
    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),
    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),

    tf.keras.layers.Dense(output_size,activation='softmax')
])
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

NUM_EPOCHS=5
model.fit(train_data,epochs=NUM_EPOCHS,validation_data=(validated_inputs,validated_targets ),verbose=2)

# 1,when width of hidden layer changed to 200,--accuracy: 0.9873 - val_loss: 0.0414 - val_accuracy: 0.9882

# 2,when depth increased( no of hidden layer) from 2 to 3 , accuracy: 0.9863 - val_loss: 0.0602 - val_accuracy: 0.9800

# 3 width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train
#hidden layer=150
 # 5 hidden layer
 # accuracy: 0.9836 - val_loss: 0.0615 - val_accuracy: 0.9828

# 4 ,sigmoid for both layer
## accuracy: 0.9670 - val_loss: 0.1095 - val_accuracy: 0.9707

## 5 try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.
## accuracy: 0.9867 - val_loss: 0.0512 - val_accuracy: 0.9847

## 6,batch 10000

##accuracy: 0.8869 - val_loss: 0.3705 - val_accuracy: 0.8982

## 7, batch 1

error

## learnin rate =0.01

## accuracy: 0.9866 - val_loss: 0.0580 - val_accuracy: 0.9832

## learning rate =0.02

## accuracy: 0.9795 - val_loss: 0.0818 - val_accuracy: 0.9788

# test data

model.evaluate(test_data)

"""### model has about 98.01 accuracy"""











